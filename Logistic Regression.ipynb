{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5860dbb-1588-4b01-8e7b-8f301cc60c1e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **Part I: Research Question**\n",
    "\n",
    "## Research Question\n",
    "\n",
    "My dataset for this predictive modeling exercise includes data on an internet service provider’s current and former subscribers, with an emphasis on customer churn (whether customers are maintaining or discontinuing their subscription to the ISP’s service).  Data analysis performed on the dataset will be aimed with this research question in mind: is there a relationship between customer lifestyle, or “social” factors, and customer churn?  Lifestyle and social factors might include variables such as age, income, and marital status, among others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c77f7e-4efb-48fb-bcb6-f936c1d19517",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Objectives and Goals\n",
    "\n",
    "Conclusions gleaned from the analysis of this data can benefit stakeholders by revealing information on which customer populations may be more likely to “churn”, or terminate their service contract with the ISP.  Such information may be used to fuel targeted advertising campaigns, special promotional offers, and other strategies related to customer retention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a054dcde-e9ef-454d-b0cc-2acea00e6744",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **Part II: Method Justification**\n",
    "\n",
    "## Assumptions of a logistic regression model\n",
    "\n",
    "The assumptions of a logistic regression model are as follows:\n",
    "\n",
    "- The Response Variable is Binary\n",
    "- The Observations are Independent\n",
    "- There is No Multicollinearity Among Explanatory Variables\n",
    "- There are No Extreme Outliers\n",
    "- There is a Linear Relationship Between Explanatory Variables and the Logit of the Response Variable\n",
    "- The Sample Size is Sufficiently Large\n",
    "\n",
    "For each of these assumptions that are violated, the potential reliability of the logistic regression model decreases. Adherence to these assumptions can be measured via tests such as Box-Tidwell, checking for extreme outliers, and VIF (Zach, 2021)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac91a96-f89b-42fc-a2c2-4ffe7deab47b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Tool Selection\n",
    "\n",
    "All code execution was carried out via Jupyter Lab, using Python 3.  I used Python as my selected programming language due to prior familiarity and broader applications when considering programming in general.  R is a very strong and robust language tool for data analysis and statistics but finds itself somewhat limited to that niche role (Insights for Professionals, 2019).  I utilized the NumPy, Pandas, and Matplotlib libraries to perform many of my data analysis tasks, as they are among the most popular Python libraries employed for this purpose and see widespread use.  Seaborn is included primarily for its better-looking boxplots, seen later in this document (Parra, 2021).  \n",
    "\n",
    "Beyond these libraries, I relied upon the Statsmodels library.  Statsmodels is one of several Python libraries that support linear and logistic regression.  I am most familiar with it due to the course material's heavy reliance upon it.  I also used the confusion_matrix and accuracy_score functions from scikit-learn's metrics module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fcdc0f-0443-4104-ab37-01711ec06cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and housekeeping\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import logit\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.graphics.mosaicplot import mosaic\n",
    "from statsmodels.genmod import families\n",
    "from statsmodels.genmod.generalized_linear_model import GLM\n",
    "from sklearn.metrics import (confusion_matrix, accuracy_score)\n",
    "sns.set_theme(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ec2e4b-af44-4bfe-adf0-ed5cfa8fb242",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Why Logistic Regression?\n",
    "\n",
    "Like linear regression, logistic regression is used to understand the relationship between one or more independent variables and a single dependent variable.  Where logistic regression differs is in the type of prediction being made; where linear regression better helps us predict measurements, logistic regression helps predict whether or not an event will occur or a particular choice will be made.  It works best when used with a dependent variable that has an “either/or” or “yes/no” response.  Utilizing multiple independent variables in a predictive model can make our predictions stronger and allows higher conviction in the reliance on those models for decision making.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bf7d80-d5c0-42e8-9a71-508c65ce12a5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **Part III: Data Preparation**\n",
    "\n",
    "## Data Preparation Goals and Data Manipulations\n",
    "\n",
    "I would like my data to include only variables relevant to my research question, and to be clean and free of missing values and duplicate rows.  It will also be important to re-express any categorical variable types with numeric values.  My first steps will be to import the complete data set and execute functions that will give me information on its size, the data types of its variables, and a peek at the data in table form.  I will then narrow the data set to a new dataframe containing only the variables I am concerned with, and then utilize functions to determine if any null values or duplicate rows exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f29e528-7912-421d-a7c6-b6a690e59646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the main dataset\n",
    "df = pd.read_csv('churn_clean.csv',dtype={'locationid':np.int64})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee414452-723a-470c-bd4a-9460566e0c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb69e5e-d54f-47b0-97a1-a8e1fa7760f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset top 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e7d10b-f12c-4f77-a0e0-ffd67333210d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim dataset to variables relevant to research question\n",
    "columns = ['Area', 'Children', 'Age', 'Income', 'Marital', 'Gender', 'Churn', 'Outage_sec_perweek', \n",
    "           'Yearly_equip_failure', 'Tenure', 'MonthlyCharge', 'Bandwidth_GB_Year']\n",
    "df_data = pd.DataFrame(df[columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e18b635-4ef3-4784-b58c-e8910d3f286e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data for null or missing values\n",
    "df_data.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7439d052-6ab0-42a7-99ba-0b77857c6fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data for duplicated rows\n",
    "df_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b6f413-cf13-4d57-8eab-1fd939d6df06",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary Statistics\n",
    "\n",
    "I can use the describe() function to display the summary statistics for the entire dataframe, as well as each variable I'll be evaluating for inclusion in the model.  I have selected the Churn variable as my dependent variable.\n",
    "\n",
    "I will also utilize histogram plots to illustrate the distribution of each numeric variable in the dataframe, and countplots for the categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce94b005-ed08-4e77-ae82-56e1ae322df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics for entire dataset - continuous variables\n",
    "df_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecd399d-018a-466d-b327-61241ba163f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics for entire dataset - categorical variables\n",
    "df_data.describe(include = object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8f41a0-8759-4f63-8453-b7dbaeee83cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize figure size settings\n",
    "plt.rcParams['figure.figsize'] = [10, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059f6bfc-77dc-44f0-b10a-b2bd499e1d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display histogram plots for distribution of continuous variables\n",
    "df_data.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b839e9-e90e-4dd5-9cd8-58824dfea86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display histogram plot and summary statistics for Bandwidth_GB_Year\n",
    "df_data['Bandwidth_GB_Year'].hist(legend = True)\n",
    "plt.show()\n",
    "df_data['Bandwidth_GB_Year'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d19512-b332-43e8-bd16-43ca49a5fb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display histogram plot and summary statistics for Children\n",
    "df_data['Children'].hist(legend = True)\n",
    "plt.show()\n",
    "df_data['Children'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28915e9e-db9e-45f2-be4f-d4a32031b44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display histogram plot and summary statistics for Age\n",
    "df_data['Age'].hist(legend = True)\n",
    "plt.show()\n",
    "df_data['Age'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdb1cbc-ea95-42b8-9e0a-60f66dcce1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display histogram plot and summary statistics for Income\n",
    "df_data['Income'].hist(legend = True)\n",
    "plt.show()\n",
    "df_data['Income'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75133ed9-3058-43a5-87a2-a1db0c697f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display histogram plot and summary statistics for Outage_sec_perweek\n",
    "df_data['Outage_sec_perweek'].hist(legend = True)\n",
    "plt.show()\n",
    "df_data['Outage_sec_perweek'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c7b81a-1849-442c-bd7d-e84e1b9598d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display histogram plot and summary statistics for Yearly_equip_failure\n",
    "df_data['Yearly_equip_failure'].hist(legend = True)\n",
    "plt.show()\n",
    "df_data['Yearly_equip_failure'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9759720e-4194-40ba-b251-fbe74c3fb9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display histogram plot and summary statistics for Tenure\n",
    "df_data['Tenure'].hist(legend = True)\n",
    "plt.show()\n",
    "df_data['Tenure'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8b6bd0-de6f-49d3-a3b8-238b033a3e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display histogram plot and summary statistics for MonthlyCharge\n",
    "df_data['MonthlyCharge'].hist(legend = True)\n",
    "plt.show()\n",
    "df_data['MonthlyCharge'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eadcb5-10f4-417c-85d5-06753d0f44f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display countplots for distribution of categorical variables\n",
    "fig, ax = plt.subplots(figsize = (20,20), ncols = 2, nrows = 2)\n",
    "sns.countplot(x='Area', data=df_data, ax = ax[0][0])\n",
    "sns.countplot(x='Marital', data=df_data, ax = ax[0][1])\n",
    "sns.countplot(x='Gender', data=df_data, ax = ax[1][0])\n",
    "sns.countplot(x='Churn', data=df_data, ax = ax[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a1ddb5-9d45-4177-a860-e9f8e3d41e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display countplot and summary statistics for Area\n",
    "sns.countplot(x='Area', data=df_data)\n",
    "plt.show()\n",
    "df_data['Area'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1916e7f7-89f9-48cf-84fb-34143bc87fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display countplot and summary statistics for Marital\n",
    "sns.countplot(x='Marital', data=df_data)\n",
    "plt.show()\n",
    "df_data['Marital'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d4c0c4-a0ce-40a4-b892-c5727d1fe776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display countplot and summary statistics for Gender\n",
    "sns.countplot(x='Gender', data=df_data)\n",
    "plt.show()\n",
    "df_data['Gender'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce93bc1-08e4-414b-b73f-c56eea768e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display countplot and summary statistics for Churn\n",
    "sns.countplot(x='Churn', data=df_data)\n",
    "plt.show()\n",
    "df_data['Churn'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c354f094-8646-4d34-b65f-4a644e085b83",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Further Preparation Steps\n",
    "\n",
    "I will make some adjustments to my data types to make my variables easier to work with.  Conversion of \"object\" types as \"category\" in particular will lend itself to a more efficient conversion of categorical variables to numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9c1977-9c8b-4954-aa4b-6556cc2ff63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reassign data types\n",
    "for col in df_data:\n",
    "    if df_data[col].dtypes == 'object':\n",
    "        df_data[col] = df_data[col].astype('category')\n",
    "    if df_data[col].dtypes == 'int64':\n",
    "        df_data[col] = df_data[col].astype(int)\n",
    "    if df_data[col].dtypes == 'float64':\n",
    "        df_data[col] = df_data[col].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f5da77-2d2d-4ffb-bc1b-5e770b75034c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset info and observe data type changes\n",
    "df_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43ca982-240f-4932-8b40-fd614fb6ab38",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Here I will use the cat.codes accessor to perform label encoding on my categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0656f973-3cd8-413b-85a8-9f2f0b33d0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cat.codes for label encoding of 4 categorical variables\n",
    "df_data['Area_cat'] = df_data['Area'].cat.codes\n",
    "df_data['Marital_cat'] = df_data['Marital'].cat.codes\n",
    "df_data['Gender_cat'] = df_data['Gender'].cat.codes\n",
    "df_data['Churn_cat'] = df_data['Churn'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaa791b-4c25-4930-a590-034c344f8684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset top 5 rows from label encoded variables\n",
    "df_data[['Area', 'Marital', 'Gender', 'Churn', 'Area_cat', 'Marital_cat', 'Gender_cat', 'Churn_cat']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19532b0-c300-4951-987b-26d8ce0ace9a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Univariate and Bivariate Visualizations\n",
    "\n",
    "Univariate analysis of each variable can be seen above in section 2 of part III, \"Data Preparation\".  I will make use of Seaborn's boxplot() function for bivariate analysis of all variables.  Each independent variable is paired against my dependent variable, \"Churn\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f39c82-4041-429b-bcf1-00834ae2c743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display boxplots for bivariate analysis of variables - dependent variable = Churn\n",
    "fig, ax = plt.subplots(figsize = (20, 20), ncols = 4, nrows = 3)\n",
    "sns.boxplot(x = 'Churn', y = 'Children', data = df_data, ax = ax[0][0])\n",
    "sns.boxplot(x = 'Churn', y = 'Age', data = df_data, ax = ax[0][1])\n",
    "sns.boxplot(x = 'Churn', y = 'Income', data = df_data, ax = ax[0][2])\n",
    "sns.boxplot(x = 'Churn', y = 'Outage_sec_perweek', data = df_data, ax = ax[0][3])\n",
    "sns.boxplot(x = 'Churn', y = 'Yearly_equip_failure', data = df_data, ax = ax[1][0])\n",
    "sns.boxplot(x = 'Churn', y = 'Tenure', data = df_data, ax = ax[1][1])\n",
    "sns.boxplot(x = 'Churn', y = 'MonthlyCharge', data = df_data, ax = ax[1][2])\n",
    "sns.boxplot(x = 'Churn', y = 'Bandwidth_GB_Year', data = df_data, ax = ax[1][3])\n",
    "sns.boxplot(x = 'Churn', y = 'Area_cat', data = df_data, ax = ax[2][0])\n",
    "sns.boxplot(x = 'Churn', y = 'Marital_cat', data = df_data, ax = ax[2][1])\n",
    "sns.boxplot(x = 'Churn', y = 'Gender_cat', data = df_data, ax = ax[2][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b50a15a-35e2-4860-a964-c13ce40abbdf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Copy of Prepared Data Set\n",
    "\n",
    "Below is the code used to export the prepared data set to CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ab6693-caea-4177-907d-5d8a9c72912c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export prepared dataframe to csv\n",
    "df_data.to_csv(r'C:\\Users\\wstul\\d208\\churn_clean_perpared.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a60cda-2f22-49ce-af3f-5cf7125b5c66",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **Part IV: Model Comparison and Analysis**\n",
    "\n",
    "## Initial Logistic Regression Model\n",
    "\n",
    "Below I will create an initial logistic regression model and display its summary info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef841b6-7e77-463e-9034-dd3e7ccadd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create initial model and display summary\n",
    "mdl_churn_vs_all = logit(\"Churn_cat ~ Area_cat + Children + Age + Income + Marital_cat + Gender_cat + Bandwidth_GB_Year + \\\n",
    "                        Outage_sec_perweek + Yearly_equip_failure + MonthlyCharge + Tenure\", data=df_data).fit()\n",
    "print(mdl_churn_vs_all.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b88c67-26e3-4f07-9b56-881a85cdd1a5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reducing the Initial Model\n",
    "\n",
    "Starting from this initial model, I will aim to reduce the model by eliminating variables not suitable for this logistic regression, using statistical analysis in my selection process.\n",
    "\n",
    "To begin I will look at some additional metrics for the current model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08f4d05-079a-4bf6-8577-590a97bc8408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the dependent and independent variables\n",
    "Xtest = df_data[['Area_cat', 'Children', 'Age', 'Income', 'Marital_cat', 'Gender_cat', 'Outage_sec_perweek', \n",
    "           'Yearly_equip_failure', 'Tenure', 'MonthlyCharge', 'Bandwidth_GB_Year']]\n",
    "ytest = df_data['Churn_cat']\n",
    "# performing predictions on the test datdaset\n",
    "yhat = mdl_churn_vs_all.predict(Xtest)\n",
    "prediction = list(map(round, yhat))\n",
    "# confusion matrix\n",
    "conf_matrix = confusion_matrix(ytest, prediction)\n",
    "print (\"Confusion Matrix : \\n\", conf_matrix)\n",
    "# accuracy score of the model\n",
    "print('Test accuracy = ', accuracy_score(ytest, prediction))\n",
    "# confusion matrix visualized\n",
    "mosaic(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5106cac3-e0ca-44a7-9af4-1246ad0de69a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "As I proceed through my reduction process, I will aim to keep the test accuracy close to the initial model's performance while minimizing additional false positives and negatives.  Higher accuracy scores are considered better, with 1.000 being the maximum.\n",
    "\n",
    "First I will generate a correlation table as a reference during the selection process, and perform a variance inflation factor analysis for all features currently in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58799071-356a-4cf7-a3aa-ebbf2624c71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996bd32e-726d-4473-b26d-34accbaebd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform variance inflation factor analysis for initial feature set\n",
    "X = df_data[['Area_cat', 'Children', 'Age', 'Income', 'Marital_cat', 'Gender_cat', 'Outage_sec_perweek', \n",
    "           'Yearly_equip_failure', 'Tenure', 'MonthlyCharge', 'Bandwidth_GB_Year']]\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['IndVar'] = X.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b04fba-6814-41e2-8e50-72304b268a25",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Right away, I can see very high VIF scores for two variables, Tenure and Bandwidth_GB_Year.  High VIF values (usually greater than 5-10) indicate a high degree of multicollinearity with other variables in the model.  This reduces the model accuracy, so I will start by dropping one of these two variables from the set and repeat my VIF analysis.\n",
    "\n",
    "As Tenure has a slightly better correlation with my dependent variable, Churn_cat, I will drop Bandwidth_GB_Year first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa47874-552d-4348-8d77-0e8699296766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 1 high VIF variable\n",
    "X = X.drop('Bandwidth_GB_Year', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf74850-0aae-4888-9d50-070281d8b88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform variance inflation factor analysis for trimmed feature set\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['IndVar'] = X.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8a3853-81f6-4605-ba70-e047d9fcf225",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The VIF scores look much better than they did, but there are still a few that are rather high.  Referring back to my correlation table, MonthlyCharge has a far greater correlation with my dependent variable than Outage_sec_perweek does, so I will drop Outage_sec_perweek and repeat the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5543eefb-d610-456d-9590-ec03f615bf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 1 high VIF variable\n",
    "X = X.drop('Outage_sec_perweek', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3982bc-399e-4a53-a3ec-d1584809b8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform variance inflation factor analysis for trimmed feature set\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['IndVar'] = X.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497e7db7-5f1a-434d-9ca4-4619cad11dee",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "I have only 2 variables remaining with VIF greater than 5.  Once again the correlation table recognizes MonthlyCharge as a better candidate for inclusion in the model, so Age will be dropped from the group of independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2efb5a-dcf4-4f16-a9b6-40b1b51d637d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 1 high VIF variable\n",
    "X = X.drop('Age', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fd20eb-da13-4c29-96ac-c7a85962b9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform variance inflation factor analysis for trimmed feature set\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['IndVar'] = X.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d29cb2-212a-40ba-94e4-cecaa27114a8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "While MonthlyCharge still has a score higher than the other remaining variables, it is still less than 10.\n",
    "\n",
    "I will create a reduced model based on my remaining variables to see how our statistics look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e826f0-f4e8-475a-bc18-1e2352476fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create first reduced model and display summary\n",
    "mdl_churn_vs_reduced = logit(\"Churn_cat ~ Area_cat + Children + Income + Marital_cat + Gender_cat + Yearly_equip_failure + MonthlyCharge + Tenure\",\n",
    "                           data=df_data).fit()\n",
    "print(mdl_churn_vs_reduced.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858668db-1eb7-47d5-a41f-8e6ee2a5da39",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "According to this summary, several variables exhibit high p-values, indicating no relationship between that variable and the dependent variable, Churn_cat.  A value greater than .05 is considered high.  I will remove these variables from the model and once again evaluate the resulting summary and statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473d32a0-8c06-4697-8473-ba1e60423540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create first reduced model and display summary\n",
    "mdl_churn_vs_features = logit(\"Churn_cat ~ MonthlyCharge + Tenure\",\n",
    "                           data=df_data).fit()\n",
    "print(mdl_churn_vs_features.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96a2332-6d32-436c-8f17-5446adeb12da",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest = df_data[['MonthlyCharge', 'Tenure']]\n",
    "ytest = df_data['Churn_cat']\n",
    "yhat = mdl_churn_vs_features.predict(Xtest)\n",
    "prediction = list(map(round, yhat))\n",
    "conf_matrix = confusion_matrix(ytest, prediction)\n",
    "print (\"Confusion Matrix : \\n\", conf_matrix)\n",
    "print('Test accuracy = ', accuracy_score(ytest, prediction))\n",
    "mosaic(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3837b3-8a54-4064-b2d1-18681eba23d1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Final Reduced Multiple Regression Model\n",
    "\n",
    "At this point, I have eliminated any sources of multicollinearity and collinearity as well as variables exhibiting p-values that exceed .05.  I will finalize the reduced model and check to see how it compares to my initial model which included all variables in the set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5bb910-27ce-4e05-bfd2-05465f73eca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final reduced model and display summary\n",
    "mdl_churn_vs_features_final = logit(\"Churn_cat ~ MonthlyCharge + Tenure\",\n",
    "                           data=df_data).fit()\n",
    "print(mdl_churn_vs_features_final.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4019e4-7473-46d4-96e3-214700746af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrix, accuracy score, and mosaic for initial model and final reduced model for comparison\n",
    "Xorig = df_data[['Area_cat', 'Children', 'Age', 'Income', 'Marital_cat', 'Gender_cat', 'Outage_sec_perweek', \n",
    "           'Yearly_equip_failure', 'Tenure', 'MonthlyCharge', 'Bandwidth_GB_Year']]\n",
    "yorig = df_data['Churn_cat']\n",
    "yhat_orig = mdl_churn_vs_all.predict(Xorig)\n",
    "prediction_orig = list(map(round, yhat_orig))\n",
    "conf_matrix_orig = confusion_matrix(yorig, prediction_orig)\n",
    "Xfinal = df_data[['MonthlyCharge', 'Tenure']]\n",
    "yfinal = df_data['Churn_cat']\n",
    "yhat_final = mdl_churn_vs_features_final.predict(Xfinal)\n",
    "prediction_final = list(map(round, yhat_final))\n",
    "conf_matrix_final = confusion_matrix(yfinal, prediction_final)\n",
    "print (\"Orignal Confusion Matrix : \\n\", conf_matrix_orig)\n",
    "print('Orignal accuracy = ', accuracy_score(yorig, prediction_orig))\n",
    "print (\"Final Confusion Matrix : \\n\", conf_matrix_final)\n",
    "print('Final accuracy = ', accuracy_score(yfinal, prediction_final))\n",
    "mosaic(conf_matrix_orig)\n",
    "mosaic(conf_matrix_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b05a377-4701-46bc-a361-05154622811e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The reduced model holds up well when compared to the initial model using the accuracy score and confusion matrix as my measurements.\n",
    "\n",
    "I will perform due diligence and check for extreme outliers to make sure my independent variables comply with the assumptions of logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7783b1fc-0a68-4479-a80f-0cec08b5b71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='MonthlyCharge',data=df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da248b1-04e6-49f1-b2b9-2cdd96c05482",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='Tenure',data=df_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1cf37b-bc95-4699-9e1a-476adcd50c0b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Neither variable has extreme outliers.\n",
    "\n",
    "While Python does not have a library supporting the Box-Tidwell test with a single line of code, I can use a combination of functions from the statsmodels package to perform the test.  The goal will be to verify that the \"MonthlyCharge:Log_MonthlyCharge\" and \"Tenure:Log_Tenure\" interactions have p-values greater than 0.05, implying that both independent variables are linearly related to the logit of the dependent variable, Churn_cat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e37942-42a9-4a82-ad8f-0983729c027a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define continuous variables\n",
    "continuous_var = ['MonthlyCharge', 'Tenure']\n",
    "\n",
    "# Add logit transform interaction terms (natural log) for continuous variables e.g.. Age * Log(Age)\n",
    "for var in continuous_var:\n",
    "    df_data[f'{var}:Log_{var}'] = df_data[var].apply(lambda x: x * np.log(x))\n",
    "\n",
    "# Keep columns related to continuous variables\n",
    "cols_to_keep = continuous_var + df_data.columns.tolist()[-len(continuous_var):]\n",
    "\n",
    "# Redefining variables to include interaction terms\n",
    "X_lt = df_data[cols_to_keep]\n",
    "y_lt = df_data['Churn_cat']\n",
    "\n",
    "# Add constant term\n",
    "X_lt_constant = sm.add_constant(X_lt, prepend=False)\n",
    "  \n",
    "# Building model and fit the data (using statsmodel's Logit)\n",
    "logit_results = GLM(y_lt, X_lt_constant, family=families.Binomial()).fit()\n",
    "\n",
    "# Display summary results\n",
    "print(logit_results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f4146e-8f8a-4935-ac7f-f4c8006212bd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data Analysis Process\n",
    "\n",
    "During my variable selection process, I relied upon trusted methods for identifying variables unsuitable for the model, such as VIF, a correlation table, and p-values.  I measured each model's performance by its accuracy score, as well as the confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2011a37-16db-4552-b8f5-363f7c355d52",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **Part V: Data Summary and Implications**\n",
    "\n",
    "## Summary of Findings\n",
    "\n",
    "The regression equation for the final reduced model is as follows:\n",
    "\n",
    "**Churn_cat ~ MonthlyCharge + Tenure**\n",
    "\n",
    "The coefficients for each variable included:\n",
    "\n",
    "**MonthlyCharge     0.0332**\n",
    "\n",
    "**Tenure           -0.0738**\n",
    "\n",
    "We can use these coefficients to determine the effect each variable will have on a customer's decision to cancel service.  MonthlyCharge has a positive coefficient, indicating the higher a customer's monthly charge is, the more likely they are to churn.  Contrarily, Tenure has a negative coefficient, which tells us that customers who retain service for longer periods of time grow less and less likely to churn.\n",
    "\n",
    "The model can provide significant data when evaluating customer retention from a practical perspective, as customers who have been with the company for a long time may be less likely to cancel service, but if the rate they are charged increases so do the chances they will churn.  The limitations of using logistic regression models for practical purposes are always present, however.  They are susceptible to overfitting and can appear to have more predictive power than they do (Robinson, 2018).  Also, as logistic regressions cannot predict continuous outcomes, their predictions contain less detail.  For instance, this model may be able to predict what makes a customer more likely to churn, but not when they might do so.\n",
    "\n",
    "---\n",
    "\n",
    "## Recommended Course of Action\n",
    "\n",
    "There are a few key takeaways based on the analysis of this model.  For each month the customer stays with the ISP they are less likely to churn, but if their monthly fee increases they are more likely to churn.  It may be beneficial to offer long-standing customers occasional discounted rates to further increase the chance they will retain service.  For newer customers who already run a higher risk of churning, increasing their rates should be avoided altogether."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400dfcb1-d7f6-4197-ab70-b39c01c6c6b5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **Part VI: Demonstration**\n",
    "\n",
    "**Panopto Video Recording**\n",
    "\n",
    "A link for the Panopto video has been provided separately.  The demonstration includes the following:\n",
    "\n",
    "•  Demonstration of the functionality of the code used for the analysis\n",
    "\n",
    "•  Identification of the version of the programming environment\n",
    "\n",
    "•  Comparison of the two multiple regression models you used in your analysis\n",
    "\n",
    "•  Interpretation of the coefficients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f451a512-1938-4e97-a362-db79a69d05a5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **Web Sources**\n",
    "\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.astype.html\n",
    "\n",
    "https://pbpython.com/categorical-encoding.html\n",
    "\n",
    "https://towardsdatascience.com/assumptions-of-logistic-regression-clearly-explained-44d85a22b290\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e762db-fcb2-423a-9cd3-40d9135f73f0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **References**\n",
    "\n",
    "\n",
    "Insights for Professionals. (2019, February 26). *5 Niche Programming Languages (And Why They're Underrated).* https://www.insightsforprofessionals.com/it/software/niche-programming-languages\n",
    "\n",
    "\n",
    "Parra, H.  (2021, April 20).  *The Data Science Trilogy.*  Towards Data Science.  https://towardsdatascience.com/the-data-science-trilogy-numpy-pandas-and-matplotlib-basics-42192b89e26\n",
    "\n",
    "\n",
    "Zach.  (2021, November 16).  *The 6 Assumptions of Logistic Regression (With Examples).*  Statology.  https://www.statology.org/assumptions-of-logistic-regression/\n",
    "\n",
    "\n",
    "Robinson, N.  (2018, June 28).  *The Disadvantages of Logistic Regression.*  The Classroom.  https://www.theclassroom.com/multivariate-statistical-analysis-2448.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
